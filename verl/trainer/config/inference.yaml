# inference.yaml

# Model configuration
embedding:
  # Model information
  model:
    name: "BAAI/bge-base-en-v1.5"  # Popular embedding model
  nnodes: 1
  n_gpus_per_node: 8
  # Output settings
  output_path: "./embeddings_output"
  
  # Processing parameters
  batch_size: 32
  max_length: 512  # Maximum number of tokens to process per input text
  pooling_method: "mean"
  normalize_embeddings: true

# Data configuration
data:
  path: "path/to/your/data.parquet"
  prompt_key: "prompt"
  train_ratio: 1.0
  train_ratio_seed: null

